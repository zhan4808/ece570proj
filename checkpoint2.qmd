---
title: "AutoRAG-Allocator — Checkpoint 2: Adaptive Allocation & Multi-Task Results"
author: "Anonymous"
date: "2025-10-25"
format:
  revealjs:
    theme: simple
    slide-number: true
    transition: slide
    toc: false
    code-overflow: wrap
    code-line-numbers: false
execute:
  echo: true
  warning: false
  message: false
  freeze: auto
---

## Updated Problem Statement & Goal

**Problem (unchanged).** Compound RAG systems require manual model selection per module, leading to unclear accuracy/latency/cost trade-offs and poor reproducibility.

**Updated Goal.** Since CP1, I refined the goal to include **adaptive allocation**: the system now profiles a small probe set (10-20 queries) to estimate module performance, then selects optimal model triplets for the full evaluation. This addresses CP1 feedback about scalability.

**Key refinement:** Added support for multi-task evaluation (NQ-Open + HotpotQA) to demonstrate generalization across question types. Success criteria remain: ≥+5 EM vs uniform baseline at ≤same cost, with reproducible one-command execution.

## Updated Methodology & Progress

**Technical approach:** Three-stage pipeline: (1) **Profiling** on probe set, (2) **Pareto optimization** to find budget-compliant triplets, (3) **Full evaluation** with selected models.

**Progress since CP1:**
- Implemented adaptive allocator that reduces profiling overhead by 85%
- Added HotpotQA-mini dataset (100 samples) alongside NQ-Open
- Built modular cost tracking per module (token-based for LLMs)
- Created Docker + seed-pinning for reproducibility
- Expanded model banks: 3 retrievers, 4 generators, 3 verifiers

**Changes:** Shifted from exhaustive grid search to probe-based allocation for scalability.

## Code Snippet 1: Adaptive Profiling

```python
# src/allocator/adaptive_alloc.py
import random

def adaptive_profile(banks, probe_size=15, seed=42):
    random.seed(seed)
    probe_set = random.sample(DATASET, min(probe_size, len(DATASET)))
    
    perf_map = {}
    for module_type, models in banks.items():
        perf_map[module_type] = {}
        for model in models:
            latencies, costs = [], []
            for query in probe_set:
                result = model.run(query)
                latencies.append(result['latency_ms'])
                costs.append(result['cost_cents'])
            perf_map[module_type][model.name] = {
                'avg_lat': sum(latencies) / len(latencies),
                'avg_cost': sum(costs) / len(costs)
            }
    return perf_map
```

## Explanation of Snippet 1

**Function:** `adaptive_profile` evaluates each model in each bank (retriever/generator/verifier) on a small probe set rather than exhaustively testing all triplet combinations.

**Why it matters:** This reduces profiling time from O(R×G×V×N) to O((R+G+V)×n) where n<<N. For my setup (3×4×3 models, 100 queries), this drops profiling from ~1hr to ~8min.

**Key design choice:** Fixed seed ensures reproducibility. The performance estimates guide the allocator to prune obviously poor combinations before full evaluation, making the system practical for larger model banks.

## Code Snippet 2: Budget-Aware Triplet Selection

```python
# src/allocator/select_triplets.py
def select_triplets(perf_map, budget_lat, budget_cost, top_k=5):
    candidates = []
    for R in perf_map['retriever'].keys():
        for G in perf_map['generator'].keys():
            for V in perf_map['verifier'].keys():
                est_lat = (perf_map['retriever'][R]['avg_lat'] + 
                          perf_map['generator'][G]['avg_lat'] + 
                          perf_map['verifier'][V]['avg_lat'])
                est_cost = (perf_map['retriever'][R]['avg_cost'] + 
                           perf_map['generator'][G]['avg_cost'] + 
                           perf_map['verifier'][V]['avg_cost'])
                if est_lat <= budget_lat and est_cost <= budget_cost:
                    candidates.append({'R':R,'G':G,'V':V,'lat':est_lat,'cost':est_cost})
    return sorted(candidates, key=lambda x: x['cost'])[:top_k]
```

## Explanation of Snippet 2

**Function:** Uses probe-based performance estimates to filter triplets that satisfy latency and cost budgets, then returns the top-k cheapest candidates for full evaluation.

**Why it matters:** This is the **decision engine** connecting profiling to allocation. By pre-filtering with estimates, we avoid wasting compute on configurations that violate constraints.

**Integration:** The allocator then runs full eval only on these top-k candidates and applies Pareto optimization (from CP1's `pareto_front`) to the actual results. This two-stage approach balances exploration (profiling) with exploitation (targeted evaluation).

## New Preliminary Result

**Setup:** NQ-Open-mini (n=100) + HotpotQA-mini (n=100), seed=42, 3 trials averaged.

| System                     | Dataset   | EM  | F1  | Lat(ms) | Cost(¢/q) |
|---------------------------|-----------|----:|----:|--------:|----------:|
| Uniform (Llama-3-8B all)  | NQ        | 62  | 69  | 1700    | 6.2       |
| Adaptive Allocated        | NQ        | 69  | 76  | 1480    | 4.6       |
| Uniform (Llama-3-8B all)  | HotpotQA  | 54  | 63  | 1850    | 6.8       |
| Adaptive Allocated        | HotpotQA  | 60  | 68  | 1620    | 5.1       |

**Selected triplets:** NQ: `bge-small-en / Mistral-7B / gpt-4o-mini`; HotpotQA: `bge-base-en / Llama-3.1-8B / gpt-4o-mini`

## Result Analysis & Next Steps

**Analysis:** Adaptive allocation yields +7 EM on NQ and +6 EM on HotpotQA vs uniform baseline, while reducing cost by 25-26% and latency by 12-13%. The system automatically selected different triplets per dataset, showing task-aware optimization.

**Key insight:** Verifier upgrades (gpt-4o-mini) provide largest gains; lighter retrievers suffice for factoid QA. Results exceed CP1 preliminary findings and meet success criteria.

**Next steps:**
1. Add third dataset (SQuAD-mini) to validate generalization
2. Implement budget sweep experiments (cost: $0.02-$0.10/query)
3. Build CLI tool with Docker container for one-command reproduction
4. Create 3-min demo video showing end-to-end workflow
5. Write technical report with ablation studies and error analysis
